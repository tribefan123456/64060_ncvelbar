---
title: "Assignment 5 64060"
author: "Nate Cvelbar"
date: "2023-11-30"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
dir.create("images")
```

```{r}
#install.packages("caret")
library(caret)

#install.packages("ISLR") # only install if needed
library(ISLR)

#install.packages("tidyverse") # only install if needed
library(tidyverse)
#install.packages("cluster") # only install if needed
library(cluster)
#install.packages("factoextra") # only install if needed
library(factoextra)
#install.packages("NbClust") # only install if needed
library(NbClust)


#install.packages("stats") # only install if needed
library(stats)

#install.packages("cluster") # only install if needed
library(cluster)

#install.packages("fpc") # only install if needed
library(fpc)

```


``` {r}
#Assignment 5
#Nate Cvelbar 
#BA-64060

#File taken online from course Assignment 5
#Loading the dataset
cereals=read.csv('C:/Users/Owner/Documents/Cereals.csv')

```

```{r}
#Remove missing values
cereals <- na.omit(cereals)
head(cereals)
```


```{r}
#Remove the name, mfr, and type columns for now since they are alphabetic. Can reference alphaC later.
alphaC = cereals[c(1:3)]
cereals <- cereals[-c(1:3)]

#Scale the data
cereals <- scale(cereals)
head(cereals)
```

```{r}
#cereals <-cbind(alphaC,cereals)
#head(cereals)
#Apply Euclidean distance to the normalized measurements
d <- dist(cereals, method="euclidean")
#Generate different clusters from the 4 specified methods
hc1 <- hclust(d, method = "single" )
hc2 <- hclust(d, method = "complete" )
hc3 <- hclust(d, method = "average" )
hc4 <- hclust(d, method = "ward" )

plot(hc1, cex = 0.6, hang = -1)
plot(hc2, cex = 0.6, hang = -1)
plot(hc3, cex = 0.6, hang = -1)
plot(hc4, cex = 0.6, hang = -1)

#Based on these dendograms, I found cluster numbers of 10, 3, 3, and 4.
#Based on these results, my initial guess for the number of clusters is 4.
#I will now use the Agnes function to look further into this.

```


```{r}
#Agnes function
hc_single <- agnes(cereals, method = "single")
hc_complete <- agnes(cereals, method = "complete")
hc_average <- agnes(cereals, method = "average")
hc_ward <- agnes(cereals, method = "ward")

#Comparing agglomerative coefficients
print(hc_single$ac)
print(hc_complete$ac)
print(hc_average$ac)
print(hc_ward$ac)
#Based on Agnes, the best method is the ward method, since its ac is highest
pltree(hc_ward, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
#Based on all the data collected, and the various dendograms and ac values, I will stick with my initial cluster quantity of 4.
```
```{r}
#Divide the data into the clusters
hc_ward1 <- hclust(d,method = "ward")
# plot dendrogram to show clusters
plot(hc_ward1, cex = 0.6)
rect.hclust(hc_ward1, k = 4, border = 1:4)
#Split into clusters
clusters = cutree((hc4), k=4) 
clust.centroid = function(i, dat, clusters) {
    ind = (clusters == i)
    colMeans(dat[ind,])
}

sapply(unique(clusters), clust.centroid, cereals, clusters)
#We know that cluster 3 contains approximately 30% of data
#Based on the information found here, we can conclude that the clusters are indeed stable, since the partition compares favorably to the overall values of the data

```


```{r}
cereals1=read.csv('C:/Users/Owner/Documents/Cereals.csv')
cereals1 <- na.omit(cereals1)
#Elementary school questions
#If we manually split out the clusters to show the proper information for each, they look as follows:
#Manually split
cluster1=cereals1[-c(58,21,29,53,59,28,52,47,8,50,35,20,14,60,23,42,57,2,45,46,54,72,39,70,40,71,41,62,17,16,63,73,22,24,34,10,33,51,9,75,76,12,68,4,1,3,55,56,64,65,66,44,61,27,69),]
cluster2=cereals1[-c(21,58,5,25,43,7,18,31,67,38,15,19,30,74,13,11,36,26,32,49,48,77,6,37,54,72,39,70,40,71,41,62,17,16,63,73,22,24,34,10,33,51,9,75,76,12,68,4,1,3,55,56,64,65,66,44,61,27,69),]
cluster3=cereals1[-c(5,25,58,43,7,18,31,67,38,15,19,30,74,13,11,36,26,32,49,48,77,6,37,29,53,59,28,52,47,8,50,35,20,14,60,23,42,57,2,45,46,4,1,3,55,56,64,65,66,44,61,27,69),]
cluster4=cereals1[-c(21,5,25,43,7,18,31,67,38,15,19,30,74,13,11,36,26,32,49,48,77,6,37,29,53,59,28,52,47,8,50,35,20,14,60,23,42,57,2,45,46,54,72,39,70,40,71,41,62,17,16,63,73,22,24,34,10,33,51,9,75,76,12,68),]

#To find a suitable solution for the school, we must have a set with at least 5 cereals (each for a different day), and with healthy cereals
#This means having cereal low in sugar and carbs, but high in vitamins, fiber, and protein.
#We could perform another analysis with greater emphasis on these values, but the current clusters already separate out a healthy section.
#It is not necessary to de-scale the data first, but it does make things easier to read 
#As for normalization, I think it is more intuitive to read the raw data here, though of course the clustering should be done as before
#Based on the specs I layed out, cluster 4 here is a great choice for the school. It checks all the boxes I listed, and also rates highly too.
```

